12. Write a program to Implementation of Linear Regression using python( Any Two Algorithm).
Description:
This experiment involves implementing Linear Regression, a fundamental supervised learning algorithm for predicting numerical values.
Algorithms to be implemented (any two):
1.	Simple Linear Regression
o	Predicts a target value based on a single input feature.
o	Fits a straight line (y = mx + b) to the data.
2.	Multiple Linear Regression
o	Predicts target values using multiple input features.
o	Extends simple linear regression into higher dimensions.
3.	Polynomial Regression (optional alternative)
o	A form of regression that models the relationship as an nth-degree polynomial.
o	Useful when data shows non-linear trends.
Objective:
Use scikit-learn to build regression models and evaluate them using metrics like Mean Squared Error (MSE) and RÂ² score.
Code:
# -------------------------------
# Linear Regression Implementation
# -------------------------------

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Sample Data (Hours Studied vs Marks Scored)
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([10, 20, 30, 40, 50])

# -------------------------------
# 1. Linear Regression using Scikit-learn
# -------------------------------
print("Linear Regression using Scikit-learn")

model = LinearRegression()
model.fit(X, y)
y_pred_sklearn = model.predict(X)

print("Slope (coefficient):", model.coef_)
print("Intercept:", model.intercept_)
print("Predictions:", y_pred_sklearn)
print("MSE:", mean_squared_error(y, y_pred_sklearn))

# -------------------------------
# 2. Linear Regression using Gradient Descent
# -------------------------------
print("\n Linear Regression using Gradient Descent")

# Convert to 1D arrays for gradient descent
X_flat = X.flatten()
n = len(X_flat)

# Hyperparameters
alpha = 0.01  # learning rate
epochs = 1000

# Initialize weights
m = 0
c = 0

# Gradient Descent Algorithm
for _ in range(epochs):
    y_pred = m * X_flat + c
    error = y_pred - y
    cost = np.sum(error ** 2) / n
    m_grad = (2 / n) * np.sum(error * X_flat)
    c_grad = (2 / n) * np.sum(error)
    m -= alpha * m_grad
    c -= alpha * c_grad

print(f"Learned slope (m): {m:.2f}")
print(f"Learned intercept (c): {c:.2f}")
y_pred_manual = m * X_flat + c
print("Predictions:", y_pred_manual)
print("MSE:", mean_squared_error(y, y_pred_manual))

# -------------------------------
# Plot both models
# -------------------------------
plt.scatter(X, y, color='blue', label='Actual Data')
plt.plot(X, y_pred_sklearn, color='green', label='Sklearn Prediction')
plt.plot(X, y_pred_manual, color='red', linestyle='--', label='Gradient Descent Prediction')
plt.title("Linear Regression: Sklearn vs Gradient Descent")
plt.xlabel("Hours Studied")
plt.ylabel("Marks Scored")
plt.legend()
plt.grid(True)
plt.show()
OUTPUT:
Linear Regression using Scikit-learn
Slope (coefficient): [10.]
Intercept: 7.105427357601002e-15
Predictions: [10. 20. 30. 40. 50.]
MSE: 8.204153414298523e-30

Linear Regression using Gradient Descent
Learned slope (m): 9.98
Learned intercept (c): 0.09
Predictions: [10.06290407 20.03880582 30.01470758 39.99060933 49.96651108]
MSE: 0.001377763797074744
